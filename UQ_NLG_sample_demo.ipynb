{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d3de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 31 17:24:35 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P40           Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    49W / 250W |      0MiB / 23040MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P40           Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    48W / 250W |      0MiB / 23040MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bea1696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /data/user-data/sa25729/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d60af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 17:24:41.788417: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-31 17:24:42.615217: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          OPTForCausalLM)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442a03da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05b781145f74da3b1be4f3f93ce9690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c615082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1442, str)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer\\'s horses slept. But Cotton wasn\\'t alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton\\'s mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer\\'s orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \\n\\n\"What are you doing, Cotton?!\" \\n\\n\"I only wanted to be more like you\". \\n\\nCotton\\'s mommy rubbed her face on Cotton\\'s and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton\\'s mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton\\'s fur was all all dry. \\n\\n\"Don\\'t ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn\\'t want that!\" \\n\\nThen Cotton thought, \"I change my mind. I like being special\". Q: What color was Cotton? A:'\n",
    "\n",
    "len(text), type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d925225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 9038, 2501, 263, 931, 29892, 297, 263, 2594, 29876, 2978, 263, 17888, 3699, 29892, 727, 10600, 263, 2217, 4796, 413, 16097, 4257, 26783, 880, 29889, 26783, 880, 10600, 1880, 701, 297, 263, 7575, 14294, 2058, 2038, 278, 2594, 29876, 988, 599, 310, 278, 2215, 1050, 29915, 29879, 15100, 12844, 415, 29889, 1205, 26783, 880, 9007, 29915, 29873, 7432, 297, 902, 2217, 3271, 2038, 278, 2594, 29876, 29892, 9360, 694, 29889, 2296, 7258, 902, 14842, 6592, 411, 902, 16823, 1357, 322, 29871, 29945, 916, 9883, 29879, 29889, 2178, 310, 902, 9883, 29879, 892, 274, 1082, 322, 1652, 3096, 29891, 29892, 763, 26783, 880, 29889, 1205, 1183, 471, 278, 871, 4796, 697, 297, 278, 14928, 29889, 450, 1791, 310, 902, 9883, 29879, 892, 599, 24841, 411, 9560, 4796, 260, 4087, 10076, 5547, 763, 26783, 880, 29915, 29879, 16823, 1357, 29889, 28265, 1422, 1754, 26783, 880, 3755, 14610, 29889, 2296, 4049, 20024, 1183, 5148, 763, 278, 1791, 310, 902, 3942, 29889, 1105, 697, 2462, 29892, 746, 26783, 880, 1476, 263, 508, 310, 278, 2030, 2215, 1050, 29915, 29879, 24841, 10675, 29892, 1183, 1304, 372, 304, 10675, 8735, 763, 963, 29889, 1932, 902, 16823, 1357, 322, 9883, 29879, 1476, 902, 896, 4687, 425, 29700, 29889, 29871, 13, 13, 29908, 5618, 526, 366, 2599, 29892, 26783, 880, 29973, 3850, 29871, 13, 13, 29908, 29902, 871, 5131, 304, 367, 901, 763, 366, 1642, 29871, 13, 13, 29907, 327, 880, 29915, 29879, 16823, 1357, 14051, 2580, 902, 3700, 373, 26783, 880, 29915, 29879, 322, 1497, 376, 9048, 26783, 880, 29892, 541, 596, 3261, 338, 577, 5051, 322, 4266, 29892, 763, 366, 29889, 1334, 723, 2360, 864, 366, 304, 367, 738, 916, 982, 1642, 1126, 411, 393, 29892, 26783, 880, 29915, 29879, 16823, 1357, 18691, 902, 701, 322, 13700, 902, 964, 263, 4802, 20968, 310, 4094, 29889, 1932, 26783, 880, 2996, 714, 1183, 471, 8735, 1449, 29889, 2439, 9883, 29879, 301, 17840, 902, 3700, 2745, 26783, 880, 29915, 29879, 3261, 471, 599, 599, 15589, 29889, 29871, 13, 13, 29908, 10310, 29915, 29873, 3926, 437, 393, 1449, 29892, 26783, 880, 3850, 896, 599, 10680, 29889, 376, 9190, 931, 366, 1795, 4473, 701, 393, 5051, 4796, 3261, 310, 15850, 322, 591, 7656, 29915, 29873, 864, 393, 3850, 29871, 13, 13, 11760, 26783, 880, 2714, 29892, 376, 29902, 1735, 590, 3458, 29889, 306, 763, 1641, 4266, 1642, 660, 29901, 1724, 2927, 471, 26783, 880, 29973, 319, 29901], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = tokenizer(text, truncation=False, padding=False)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0175e53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.tokenization_utils_base.BatchEncoding, list, list)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample), type(sample['input_ids']), type(sample['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd02107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 397)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample['input_ids']), len(sample['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ee2d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 397]), torch.Size([1, 397]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(sample['input_ids'])\n",
    "input_ids = input_ids[None, :]\n",
    "atten_mask = torch.tensor(sample['attention_mask'])\n",
    "atten_mask = atten_mask[None, :]\n",
    "\n",
    "input_ids.size(), atten_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd4191f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length = input_ids.shape[1]\n",
    "input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b33dac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LlamaTokenizerFast'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8259dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user-data/sa25729/myenv1/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/data/user-data/sa25729/myenv1/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/data/user-data/sa25729/myenv1/lib/python3.8/site-packages/transformers/generation/utils.py:1535: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _generate_config(tokenizer):\n",
    "\n",
    "#     if tokenizer.__class__.__name__ == 'LlamaTokenizerFast':\n",
    "#         eos_token_id = [tokenizer.encode(_)[-1] for _ in ['.', '\\n']] + [29889]  # seems to be '.' as well\n",
    "#         #eos_token_id = [tokenizer(_)['input_ids'] for _ in ['\\n', ',', '.']]\n",
    "#     elif tokenizer.__class__.__name__ == 'GPT2Tokenizer':\n",
    "#         eos_token_id = [tokenizer.encode(_)[1] for _ in ['.', '\\n']]\n",
    "#     else:\n",
    "#         raise NotImplementedError\n",
    "\n",
    "    eos_token_id = [tokenizer.encode(_)[-1] for _ in ['.', '\\n']] + [29889]  # seems to be '.' as well\n",
    "    eos_token_id += [tokenizer.eos_token_id]\n",
    "    question_framing_ids = ['Question:', ' Question:', '\\n', 'Answer:', ' Answer:', 'Q:']\n",
    "    # Follows Kuhn et al 2023 as Llama does not have CoQA\n",
    "    question_framing_ids = [[tokenizer(eos_token)['input_ids'][1]] for eos_token in question_framing_ids]\n",
    "    return dict(eos_token_id=eos_token_id, bad_words_ids=question_framing_ids)\n",
    "\n",
    "def get_generation_config(input_ids, tokenizer):\n",
    "    assert len(input_ids.shape) == 2\n",
    "    max_length_of_generated_sequence = 256\n",
    "#     if data_name == 'triviaqa':\n",
    "#         generation_config = triviaqa._generate_config(tokenizer)\n",
    "#     if data_name == 'coqa':\n",
    "#         generation_config = coqa._generate_config(tokenizer)\n",
    "#     if data_name == 'nq_open':\n",
    "#         generation_config = nq_open._generate_config(tokenizer)\n",
    "\n",
    "    generation_config = _generate_config(tokenizer)\n",
    "    generation_config['max_new_tokens'] = max_length_of_generated_sequence\n",
    "    generation_config['early_stopping'] = True\n",
    "    # https://jaketae.github.io/study/gpt2/#setup\n",
    "    generation_config['pad_token_id'] = tokenizer.eos_token_id\n",
    "    return generation_config\n",
    "\n",
    "generation_config = get_generation_config(input_ids, tokenizer)\n",
    "generation_config = transformers.GenerationConfig(**generation_config)\n",
    "\n",
    "most_likely_generations = model.generate(input_ids, attention_mask = atten_mask,\n",
    "                                                    num_beams=1,\n",
    "                                                    do_sample=False,\n",
    "                                                    generation_config=generation_config).cpu()[0, input_length:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b9595e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([26783,   880,   471,  4796, 29889])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Cot', 'ton', 'was', 'white', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(most_likely_generations)\n",
    "# most_likely_generations tensor([26783,   880,   471,  4796, 29889])\n",
    "\n",
    "generated_texts = [tokenizer.decode(_, skip_special_tokens=True) for _ in most_likely_generations]\n",
    "generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c01902d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[12317,  9094, 29991,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [21431, 17729,  9654, 29892, 26783,   880,   338,   263,  4266,  9654,\n",
       "          29889],\n",
       "         [ 8037, 29889,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [ 8037, 29889,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2]]),\n",
       " tensor([[ 8037, 29991,  1732,   597,  6477,   687,  2550,  2909, 29889,     2,\n",
       "              2,     2],\n",
       "         [26783,   880,   471,  2307,   278,  1021,   408,  1183,  2337,   471,\n",
       "          29991,    13],\n",
       "         [  319,  9495, 29889,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2],\n",
       "         [26783,   880,  3939,   515,  4796,   304, 24841, 29889,     2,     2,\n",
       "              2,     2]]),\n",
       " tensor([[26783,   880,   471, 24841, 29889,     2,     2,     2,     2,     2],\n",
       "         [ 2296,   471,  4796,   322,  1407,  1652,  3096, 29891, 29991,    13],\n",
       "         [ 8037, 29991,    13,     2,     2,     2,     2,     2,     2,     2],\n",
       "         [ 2296,  7743,   902,  5828,   411,   263, 15007,  2135, 29889,     2]]),\n",
       " tensor([[ 2296,   471,  9475,  1422, 11955, 29991,    13,     2,     2,     2,\n",
       "              2,     2],\n",
       "         [ 8037,  4248,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2],\n",
       "         [12317,  9094, 29889,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2],\n",
       "         [26783,   880, 29973, 26783,   880,   338,   278,  2927,   310, 20118,\n",
       "            880, 29889]]),\n",
       " tensor([[ 8037, 20118,   880, 29991,    13,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2],\n",
       "         [ 2296,   471,  4628,   322,  4796, 29991,    13,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2],\n",
       "         [ 2296,   471,  4796, 29991,  1205, 26783,   880,   471,   884,   451,\n",
       "           2337,  4796,   278,   982,   278,  1791,   310,   902,  3942,   471,\n",
       "           4796, 29889],\n",
       "         [ 8037, 29991,    13,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations = []\n",
    "num_gens = 20\n",
    "max_num_gen_once=4\n",
    "while num_gens > 0:\n",
    "    _ =  model.generate(input_ids, attention_mask=atten_mask,\n",
    "                             num_beams=1, num_return_sequences=min(max_num_gen_once, num_gens),\n",
    "                             do_sample=True, top_p= 1, top_k= 0,\n",
    "                             temperature=1, generation_config=generation_config,\n",
    "                             )\n",
    "    generations.append(_[:, input_length:].cpu())\n",
    "    num_gens -= len(_)\n",
    "    \n",
    "generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac0212d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Cot', 'ton', 'was', 'orange', '.', '', '', '', '', ''], 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_gen = generations[2][0]\n",
    "generated_sample = [tokenizer.decode(_, skip_special_tokens=True) for _ in sample_gen]\n",
    "generated_sample, len(sample_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35d49653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['She',\n",
       "  'was',\n",
       "  'white',\n",
       "  '!',\n",
       "  'But',\n",
       "  'Cot',\n",
       "  'ton',\n",
       "  'was',\n",
       "  'also',\n",
       "  'not',\n",
       "  'always',\n",
       "  'white',\n",
       "  'the',\n",
       "  'way',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'her',\n",
       "  'family',\n",
       "  'was',\n",
       "  'white',\n",
       "  '.'],\n",
       " 22)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_gen = generations[4][2]\n",
    "generated_sample = [tokenizer.decode(_, skip_special_tokens=True) for _ in sample_gen]\n",
    "generated_sample, len(sample_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93bcafe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b7f93e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2445754/4149883299.py:1: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
      "  generations = torch.nested.nested_tensor(generations).to_padded_tensor(tokenizer.eos_token_id)\n"
     ]
    }
   ],
   "source": [
    "generations = torch.nested.nested_tensor(generations).to_padded_tensor(tokenizer.eos_token_id)\n",
    "generations = generations.reshape(-1, generations.shape[-1])[:20]\n",
    "generated_texts = [tokenizer.decode(_, skip_special_tokens=True) for _ in generations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbe85836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WHITE!',\n",
       " 'Rainbow blank, Cotton is a special blank.',\n",
       " 'White.',\n",
       " 'White.',\n",
       " 'White! http://fortunecookbook.',\n",
       " 'Cotton was already the same as she always was!\\n',\n",
       " 'A mouse.',\n",
       " 'Cotton changed from white to orange.',\n",
       " 'Cotton was orange.',\n",
       " 'She was white and very fluffy!\\n',\n",
       " 'White!\\n',\n",
       " 'She finished her story with a snowball.',\n",
       " 'She was eight different colors!\\n',\n",
       " 'White :)',\n",
       " 'WHITE.',\n",
       " 'Cotton? Cotton is the color of cotton.',\n",
       " 'White cotton!\\n',\n",
       " 'She was black and white!\\n',\n",
       " 'She was white! But Cotton was also not always white the way the rest of her family was white.',\n",
       " 'White!\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbce5ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences 1 [{'prompt': tensor([[    1,  9038,  2501,   263,   931, 29892,   297,   263,  2594, 29876,\n",
      "          2978,   263, 17888,  3699, 29892,   727, 10600,   263,  2217,  4796,\n",
      "           413, 16097,  4257, 26783,   880, 29889, 26783,   880, 10600,  1880,\n",
      "           701,   297,   263,  7575, 14294,  2058,  2038,   278,  2594, 29876,\n",
      "           988,   599,   310,   278,  2215,  1050, 29915, 29879, 15100, 12844,\n",
      "           415, 29889,  1205, 26783,   880,  9007, 29915, 29873,  7432,   297,\n",
      "           902,  2217,  3271,  2038,   278,  2594, 29876, 29892,  9360,   694,\n",
      "         29889,  2296,  7258,   902, 14842,  6592,   411,   902, 16823,  1357,\n",
      "           322, 29871, 29945,   916,  9883, 29879, 29889,  2178,   310,   902,\n",
      "          9883, 29879,   892,   274,  1082,   322,  1652,  3096, 29891, 29892,\n",
      "           763, 26783,   880, 29889,  1205,  1183,   471,   278,   871,  4796,\n",
      "           697,   297,   278, 14928, 29889,   450,  1791,   310,   902,  9883,\n",
      "         29879,   892,   599, 24841,   411,  9560,  4796,   260,  4087, 10076,\n",
      "          5547,   763, 26783,   880, 29915, 29879, 16823,  1357, 29889, 28265,\n",
      "          1422,  1754, 26783,   880,  3755, 14610, 29889,  2296,  4049, 20024,\n",
      "          1183,  5148,   763,   278,  1791,   310,   902,  3942, 29889,  1105,\n",
      "           697,  2462, 29892,   746, 26783,   880,  1476,   263,   508,   310,\n",
      "           278,  2030,  2215,  1050, 29915, 29879, 24841, 10675, 29892,  1183,\n",
      "          1304,   372,   304, 10675,  8735,   763,   963, 29889,  1932,   902,\n",
      "         16823,  1357,   322,  9883, 29879,  1476,   902,   896,  4687,   425,\n",
      "         29700, 29889, 29871,    13,    13, 29908,  5618,   526,   366,  2599,\n",
      "         29892, 26783,   880, 29973,  3850, 29871,    13,    13, 29908, 29902,\n",
      "           871,  5131,   304,   367,   901,   763,   366,  1642, 29871,    13,\n",
      "            13, 29907,   327,   880, 29915, 29879, 16823,  1357, 14051,  2580,\n",
      "           902,  3700,   373, 26783,   880, 29915, 29879,   322,  1497,   376,\n",
      "          9048, 26783,   880, 29892,   541,   596,  3261,   338,   577,  5051,\n",
      "           322,  4266, 29892,   763,   366, 29889,  1334,   723,  2360,   864,\n",
      "           366,   304,   367,   738,   916,   982,  1642,  1126,   411,   393,\n",
      "         29892, 26783,   880, 29915, 29879, 16823,  1357, 18691,   902,   701,\n",
      "           322, 13700,   902,   964,   263,  4802, 20968,   310,  4094, 29889,\n",
      "          1932, 26783,   880,  2996,   714,  1183,   471,  8735,  1449, 29889,\n",
      "          2439,  9883, 29879,   301, 17840,   902,  3700,  2745, 26783,   880,\n",
      "         29915, 29879,  3261,   471,   599,   599, 15589, 29889, 29871,    13,\n",
      "            13, 29908, 10310, 29915, 29873,  3926,   437,   393,  1449, 29892,\n",
      "         26783,   880,  3850,   896,   599, 10680, 29889,   376,  9190,   931,\n",
      "           366,  1795,  4473,   701,   393,  5051,  4796,  3261,   310, 15850,\n",
      "           322,   591,  7656, 29915, 29873,   864,   393,  3850, 29871,    13,\n",
      "            13, 11760, 26783,   880,  2714, 29892,   376, 29902,  1735,   590,\n",
      "          3458, 29889,   306,   763,  1641,  4266,  1642,   660, 29901,  1724,\n",
      "          2927,   471, 26783,   880, 29973,   319, 29901]]), 'id': '3dr23u6we5exclen4th8uq9rb42tel_0', 'question': 'What color was Cotton?', 'answer': 'white', 'additional_answers': [], 'most_likely_generation_ids': tensor([26783,   880,   471,  4796, 29889]), 'generations_ids': tensor([[12317,  9094, 29991,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [21431, 17729,  9654, 29892, 26783,   880,   338,   263,  4266,  9654,\n",
      "         29889,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [ 8037, 29889,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [ 8037, 29889,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [ 8037, 29991,  1732,   597,  6477,   687,  2550,  2909, 29889,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [26783,   880,   471,  2307,   278,  1021,   408,  1183,  2337,   471,\n",
      "         29991,    13,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [  319,  9495, 29889,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [26783,   880,  3939,   515,  4796,   304, 24841, 29889,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [26783,   880,   471, 24841, 29889,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [ 2296,   471,  4796,   322,  1407,  1652,  3096, 29891, 29991,    13,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [ 8037, 29991,    13,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [ 2296,  7743,   902,  5828,   411,   263, 15007,  2135, 29889,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [ 2296,   471,  9475,  1422, 11955, 29991,    13,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [ 8037,  4248,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [12317,  9094, 29889,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [26783,   880, 29973, 26783,   880,   338,   278,  2927,   310, 20118,\n",
      "           880, 29889,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [ 8037, 20118,   880, 29991,    13,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [ 2296,   471,  4628,   322,  4796, 29991,    13,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2],\n",
      "        [ 2296,   471,  4796, 29991,  1205, 26783,   880,   471,   884,   451,\n",
      "          2337,  4796,   278,   982,   278,  1791,   310,   902,  3942,   471,\n",
      "          4796, 29889],\n",
      "        [ 8037, 29991,    13,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2]]), 'most_likely_generation': 'Cotton was white.', 'generations': ['WHITE!', 'Rainbow blank, Cotton is a special blank.', 'White.', 'White.', 'White! http://fortunecookbook.', 'Cotton was already the same as she always was!\\n', 'A mouse.', 'Cotton changed from white to orange.', 'Cotton was orange.', 'She was white and very fluffy!\\n', 'White!\\n', 'She finished her story with a snowball.', 'She was eight different colors!\\n', 'White :)', 'WHITE.', 'Cotton? Cotton is the color of cotton.', 'White cotton!\\n', 'She was black and white!\\n', 'She was white! But Cotton was also not always white the way the rest of her family was white.', 'White!\\n']}]\n"
     ]
    }
   ],
   "source": [
    "# remember the data\n",
    "# id 3dr23u6we5exclen4th8uq9rb42tel_0\n",
    "# que What color was Cotton?\n",
    "# ans white\n",
    "\n",
    "sequences = []\n",
    "curr_seq = dict(\n",
    "    prompt= input_ids,\n",
    "    id='3dr23u6we5exclen4th8uq9rb42tel_0',\n",
    "    question= 'What color was Cotton?',\n",
    "    answer= 'white',\n",
    "    additional_answers= [],\n",
    ")\n",
    "curr_seq.update(\n",
    "    dict(\n",
    "        most_likely_generation_ids = most_likely_generations,\n",
    "        generations_ids=generations,\n",
    "    )\n",
    ")\n",
    "curr_seq.update(\n",
    "    dict(\n",
    "        most_likely_generation=tokenizer.decode(curr_seq['most_likely_generation_ids'], skip_special_tokens=True),\n",
    "        generations=generated_texts,\n",
    "    )\n",
    ")\n",
    "\n",
    "# curr_seq['additional_answers'] = [x[0] for x in additional_answers]\n",
    "\n",
    "sequences.append(curr_seq)\n",
    "print('sequences', len(sequences), sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49beb62b",
   "metadata": {},
   "source": [
    "Calculate Uncertainty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f4499ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install rouge_score\n",
    "!pip install -q evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68295f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6244c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_data = dict(prompt = \"Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \\n\\n'What are you doing, Cotton?!' \\n\\n'I only wanted to be more like you'. \\n\\nCotton's mommy rubbed her face on Cotton's and said 'Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way'. And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \\n\\n'Don't ever do that again, Cotton!' they all cried. 'Next time you might mess up that pretty white fur of yours and we wouldn\\'t want that!' \\n\\nThen Cotton thought, 'I change my mind. I like being special.\",\n",
    "#                 id = '3dr23u6we5exclen4th8uq9rb42tel_0',\n",
    "#                 question = 'What color was Cotton?', \n",
    "#                  answer = 'white', \n",
    "#                 generations = {'text_cleaned': [\"She was herself A: She wasn't orange and she wasn't white.\", '**A Special one**', \"I don't tell stories like I'm your teacher in Kindergarten.\", 'Carrot!', \"Cotton's hair was orange.\", 'Cotton was white... just like Cotton was!', 'Wherever she was needed.', 'Cotton', 'Really Canary', 'A BROWN paint.', 'She was still herself.', 'She was a special, white kitten!', 'She was white!', 'White!', 'Cotton since as you may already have observed by now, it really was cotton.', 'White with stripes', 'Cotton is still white.', 'White.', \"She was white, like she was supposed to be! Did you enjoy my story? Related stories that may interest you? Here they are! do you have brains every krusty needs long noodles? and here's a hint they're made of skeletons.\", 'White!'],\n",
    "#                                 'text':[\"She was herself A: She wasn't orange and she wasn't white.\", '**A Special one**', \"I don't tell stories like I'm your teacher in Kindergarten.\", 'Carrot!', \"Cotton's hair was orange.\", 'Cotton was white... just like Cotton was!', 'Wherever she was needed.', 'Cotton\\n', 'Really Canary', 'A BROWN paint.', 'She was still herself.', 'She was a special, white kitten!\\n', 'She was white!', 'White!', 'Cotton since as you may already have observed by now, it really was cotton.', 'White with stripes\\n', 'Cotton is still white.', 'White.', \"She was white, like she was supposed to be! Did you enjoy my story? Related stories that may interest you? Here they are! do you have brains every krusty needs long noodles? and here's a hint they're made of skeletons.\", 'White!\\n']},\n",
    "#                 )\n",
    "\n",
    "demo_data = dict(prompt = text,\n",
    "                id = '3dr23u6we5exclen4th8uq9rb42tel_0',\n",
    "                question = ['What color was Cotton?'], \n",
    "                 answer = ['white'], \n",
    "                generations = {'text_cleaned': [\"She was herself A: She wasn't orange and she wasn't white.\", '**A Special one**', \"I don't tell stories like I'm your teacher in Kindergarten.\", 'Carrot!', \"Cotton's hair was orange.\", 'Cotton was white... just like Cotton was!', 'Wherever she was needed.', 'Cotton', 'Really Canary', 'A BROWN paint.', 'She was still herself.', 'She was a special, white kitten!', 'She was white!', 'White!', 'Cotton since as you may already have observed by now, it really was cotton.', 'White with stripes', 'Cotton is still white.', 'White.', \"She was white, like she was supposed to be! Did you enjoy my story? Related stories that may interest you? Here they are! do you have brains every krusty needs long noodles? and here's a hint they're made of skeletons.\", 'White!'],\n",
    "                                'text':[\"She was herself A: She wasn't orange and she wasn't white.\", '**A Special one**', \"I don't tell stories like I'm your teacher in Kindergarten.\", 'Carrot!', \"Cotton's hair was orange.\", 'Cotton was white... just like Cotton was!', 'Wherever she was needed.', 'Cotton\\n', 'Really Canary', 'A BROWN paint.', 'She was still herself.', 'She was a special, white kitten!\\n', 'She was white!', 'White!', 'Cotton since as you may already have observed by now, it really was cotton.', 'White with stripes\\n', 'Cotton is still white.', 'White.', \"She was white, like she was supposed to be! Did you enjoy my story? Related stories that may interest you? Here they are! do you have brains every krusty needs long noodles? and here's a hint they're made of skeletons.\", 'White!\\n']},\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2338f5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_temp = demo_data['generations']\n",
    "type(demo_temp['text_cleaned']), type(demo_data['answer']), len(demo_data['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "200ba62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(demo_data['generations']['text_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ba7f715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"She was herself A: She wasn't orange and she wasn't white.\",\n",
       "  '**A Special one**',\n",
       "  \"I don't tell stories like I'm your teacher in Kindergarten.\",\n",
       "  'Carrot!',\n",
       "  \"Cotton's hair was orange.\",\n",
       "  'Cotton was white... just like Cotton was!',\n",
       "  'Wherever she was needed.',\n",
       "  'Cotton',\n",
       "  'Really Canary',\n",
       "  'A BROWN paint.',\n",
       "  'She was still herself.',\n",
       "  'She was a special, white kitten!',\n",
       "  'She was white!',\n",
       "  'White!',\n",
       "  'Cotton since as you may already have observed by now, it really was cotton.',\n",
       "  'White with stripes',\n",
       "  'Cotton is still white.',\n",
       "  'White.',\n",
       "  \"She was white, like she was supposed to be! Did you enjoy my story? Related stories that may interest you? Here they are! do you have brains every krusty needs long noodles? and here's a hint they're made of skeletons.\",\n",
       "  'White!'],\n",
       " 20)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_temp['text_cleaned'], len(demo_temp['text_cleaned'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda1dfd6",
   "metadata": {},
   "source": [
    "RougeL score to get uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "047782de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions <class 'list'> [\"she was herself a: she wasn't orange and she wasn't white.\"]\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.14285714285714288, 'rouge2': 0.0, 'rougeL': 0.14285714285714288}\n",
      "predictions <class 'list'> ['**a special one**']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
      "predictions <class 'list'> [\"i don't tell stories like i'm your teacher in kindergarten.\"]\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
      "predictions <class 'list'> ['carrot!']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
      "predictions <class 'list'> [\"cotton's hair was orange.\"]\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
      "predictions <class 'list'> ['cotton was white... just like cotton was!']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.25, 'rouge2': 0.0, 'rougeL': 0.25}\n",
      "predictions <class 'list'> ['wherever she was needed.']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
      "predictions <class 'list'> ['cotton']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
      "predictions <class 'list'> ['really canary']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
      "predictions <class 'list'> ['a brown paint.']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
      "predictions <class 'list'> ['she was still herself.']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
      "predictions <class 'list'> ['she was a special, white kitten!']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.2857142857142857, 'rouge2': 0.0, 'rougeL': 0.2857142857142857}\n",
      "predictions <class 'list'> ['she was white!']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.5, 'rouge2': 0.0, 'rougeL': 0.5}\n",
      "predictions <class 'list'> ['white!']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['cotton since as you may already have observed by now, it really was cotton.']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
      "predictions <class 'list'> ['white with stripes']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.5, 'rouge2': 0.0, 'rougeL': 0.5}\n",
      "predictions <class 'list'> ['cotton is still white.']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.4, 'rouge2': 0.0, 'rougeL': 0.4}\n",
      "predictions <class 'list'> ['white.']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> [\"she was white, like she was supposed to be! did you enjoy my story? related stories that may interest you? here they are! do you have brains every krusty needs long noodles? and here's a hint they're made of skeletons.\"]\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 0.0, 'rouge1': 0.046511627906976744, 'rouge2': 0.0, 'rougeL': 0.046511627906976744}\n",
      "predictions <class 'list'> ['white!']\n",
      "references <class 'list'> ['white']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge', keep_in_memory=True)\n",
    "exact_match_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "def _compare_generated_text_to_answers(pred_txt, reference_answers):\n",
    "    pred_txt = pred_txt.lstrip().lower()\n",
    "    rouge_types = ['rouge1', 'rouge2', 'rougeL']\n",
    "    sequence_dict = {_:0. for _ in ['exact_match'] + rouge_types}\n",
    "    unique_reference_answers = set([_.lstrip().lower() for _ in reference_answers])\n",
    "    for answer in unique_reference_answers:\n",
    "        predictions = [pred_txt]\n",
    "        print('predictions', type(predictions), predictions)\n",
    "        references = [answer]\n",
    "        print('references', type(references), references)\n",
    "        results = exact_match_metric.compute(predictions=predictions,\n",
    "                                             references=references,\n",
    "                                             ignore_case=True,\n",
    "                                             ignore_punctuation=True)\n",
    "        sequence_dict['exact_match'] = max(results['exact_match'], sequence_dict['exact_match'])\n",
    "        rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "        for rouge_type in rouge_types:\n",
    "            sequence_dict[rouge_type] = max(rouge_results[rouge_type], sequence_dict[rouge_type])\n",
    "    return sequence_dict\n",
    "\n",
    "all_sequences = []\n",
    "all_rougeL = []\n",
    "for i in range(len(demo_temp['text_cleaned'])):\n",
    "    sequences_rouge = _compare_generated_text_to_answers(demo_temp['text_cleaned'][i], demo_data['answer'])\n",
    "    print(sequences_rouge)\n",
    "    all_sequences.append(sequences_rouge)\n",
    "    all_rougeL.append(sequences_rouge['rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "623ec78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'exact_match': 0.0,\n",
       "  'rouge1': 0.14285714285714288,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.14285714285714288},\n",
       " {'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'exact_match': 0.0, 'rouge1': 0.25, 'rouge2': 0.0, 'rougeL': 0.25},\n",
       " {'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'exact_match': 0.0,\n",
       "  'rouge1': 0.2857142857142857,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.2857142857142857},\n",
       " {'exact_match': 0.0, 'rouge1': 0.5, 'rouge2': 0.0, 'rougeL': 0.5},\n",
       " {'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0},\n",
       " {'exact_match': 0.0, 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'exact_match': 0.0, 'rouge1': 0.5, 'rouge2': 0.0, 'rougeL': 0.5},\n",
       " {'exact_match': 0.0, 'rouge1': 0.4, 'rouge2': 0.0, 'rougeL': 0.4},\n",
       " {'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0},\n",
       " {'exact_match': 0.0,\n",
       "  'rouge1': 0.046511627906976744,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.046511627906976744},\n",
       " {'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6b70c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14285714285714288,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2857142857142857,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.4,\n",
       " 1.0,\n",
       " 0.046511627906976744,\n",
       " 1.0]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rougeL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0b471f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2562541528239203"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rouge_score = np.array(all_rougeL).mean()\n",
    "rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba89eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "308177d1",
   "metadata": {},
   "source": [
    "Eccentricity for negtive confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02ca245e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def jaccard_one(all_answers):\n",
    "    all_answers = [set(ans.lower().split()) for ans in all_answers]\n",
    "    ret = np.eye(len(all_answers))\n",
    "    for i, ans_i in enumerate(all_answers):\n",
    "        for j, ans_j in enumerate(all_answers[i+1:], i+1):\n",
    "            ret[i,j] = ret[j,i] = len(ans_i.intersection(ans_j)) / max(len(ans_i.union(ans_j)),1)\n",
    "            \n",
    "    return ret\n",
    "\n",
    "ret = jaccard_one(demo_temp['text_cleaned'])\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "191030c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpetralClusteringFromLogits:\n",
    "    def __init__(self,\n",
    "                 affinity_mode='disagreement_w',\n",
    "                 eigv_threshold=0.9,\n",
    "                 cluster=True,\n",
    "                 temperature=3., adjust=False) -> None:\n",
    "        self.affinity_mode = affinity_mode\n",
    "        self.eigv_threshold = eigv_threshold\n",
    "        self.rs = 0\n",
    "        self.cluster = cluster\n",
    "        self.temperature = temperature\n",
    "        self.adjust = adjust\n",
    "        if affinity_mode == 'jaccard':\n",
    "            assert self.temperature is None\n",
    "\n",
    "    def get_laplacian(self, logits):\n",
    "        W = get_affinity_mat(logits, mode=self.affinity_mode, temp=self.temperature)\n",
    "        L = get_L_mat(W, symmetric=True)\n",
    "        return L\n",
    "\n",
    "    def get_eigvs(self, logits):\n",
    "        L = self.get_laplacian(logits)\n",
    "        return (1-get_eig(L)[0])\n",
    "\n",
    "    def __call__(self, logits, cluster=None):\n",
    "        if cluster is None: cluster = self.cluster\n",
    "        L = self.get_laplacian(logits)\n",
    "        if not cluster:\n",
    "            return (1-get_eig(L)[0]).clip(0 if self.adjust else -1).sum()\n",
    "        eigvals, eigvecs = get_eig(L, thres=self.eigv_threshold)\n",
    "        k = eigvecs.shape[1]\n",
    "        self.rs += 1\n",
    "        kmeans = KMeans(n_clusters=k, random_state=self.rs, n_init='auto').fit(eigvecs)\n",
    "        return kmeans.labels_\n",
    "\n",
    "    def clustered_entropy(self, logits):\n",
    "        from scipy.stats import entropy\n",
    "        labels = self(logits, cluster=True)\n",
    "        P = torch.softmax(logits, dim=-1)[:, :, 2].cpu().numpy()\n",
    "        pi = find_equidist(P)\n",
    "        clustered_pi = pd.Series(pi).groupby(labels).sum().values\n",
    "        return entropy(clustered_pi)\n",
    "\n",
    "    def eig_entropy(self, logits):\n",
    "        W = get_affinity_mat(logits, mode=self.affinity_mode, temp=self.temperature)\n",
    "        L = get_L_mat(W, symmetric=True)\n",
    "        eigs = get_eig(L, eps=1e-4)[0] / W.shape[0]\n",
    "        return np.exp(- (eigs * np.nan_to_num(np.log(eigs))).sum())\n",
    "\n",
    "    def proj(self, logits):\n",
    "        W = get_affinity_mat(logits, mode=self.affinity_mode, temp=self.temperature)\n",
    "        L = get_L_mat(W, symmetric=True)\n",
    "        eigvals, eigvecs = get_eig(L, thres=self.eigv_threshold)\n",
    "        return eigvecs\n",
    "\n",
    "    def kmeans(self, eigvecs):\n",
    "        k = eigvecs.shape[1]\n",
    "        self.rs += 1\n",
    "        kmeans = KMeans(n_clusters=k, random_state=self.rs, n_init='auto').fit(eigvecs)\n",
    "        return kmeans.labels_\n",
    "\n",
    "def umap_visualization(eigvecs, labels):\n",
    "    # perform umap visualization on the eigenvectors\n",
    "    import umap\n",
    "    reducer = umap.UMAP()\n",
    "    embedding = reducer.fit_transform(eigvecs)\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], c=labels)\n",
    "    return embedding\n",
    "\n",
    "clusterer = SpetralClusteringFromLogits(affinity_mode='agreement_w', eigv_threshold=0.9,\n",
    "                                                       cluster=False, temperature=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5af418a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mapping': [11,\n",
       "  0,\n",
       "  8,\n",
       "  2,\n",
       "  7,\n",
       "  6,\n",
       "  15,\n",
       "  3,\n",
       "  9,\n",
       "  1,\n",
       "  12,\n",
       "  10,\n",
       "  13,\n",
       "  17,\n",
       "  5,\n",
       "  16,\n",
       "  4,\n",
       "  18,\n",
       "  14,\n",
       "  17],\n",
       " 'sim_mat': tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "          [-0.3503,  1.4741, -0.9068],\n",
       "          [ 0.4314,  0.4489, -1.0398],\n",
       "          ...,\n",
       "          [-0.6590,  2.0907, -0.9428],\n",
       "          [-0.3062,  0.5733, -0.2908],\n",
       "          [-0.2184,  0.4827, -0.3478]],\n",
       " \n",
       "         [[-0.9413,  0.9582,  0.1809],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 1.7362, -0.3184, -1.7784],\n",
       "          ...,\n",
       "          [ 1.9157, -0.3024, -1.8874],\n",
       "          [ 2.9792, -1.1658, -1.9188],\n",
       "          [ 3.0403, -1.1253, -2.0292]],\n",
       " \n",
       "         [[-0.5741,  0.4342,  0.1214],\n",
       "          [ 1.2542, -0.4389, -1.2263],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.7801,  0.0574, -1.0693],\n",
       "          [ 1.1383, -0.5187, -0.9054],\n",
       "          [ 1.3437, -0.7421, -0.8874]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.7413,  0.7301,  0.0792],\n",
       "          [ 1.2882, -0.1700, -1.4613],\n",
       "          [ 1.4731, -0.1073, -1.7052],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [-1.2190,  0.5205,  0.8077],\n",
       "          [-0.9970,  0.4188,  0.6423]],\n",
       " \n",
       "         [[-0.8831,  1.1258, -0.0299],\n",
       "          [ 2.7944, -1.0997, -1.9701],\n",
       "          [ 2.8198, -1.0252, -2.0587],\n",
       "          ...,\n",
       "          [-1.4542,  3.6309, -0.9379],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [-1.5950, -0.4790,  2.1288]],\n",
       " \n",
       "         [[-0.4027,  1.0573, -0.5352],\n",
       "          [ 2.4884, -0.7383, -2.0796],\n",
       "          [ 2.9633, -0.9536, -2.2608],\n",
       "          ...,\n",
       "          [-1.1934,  3.4397, -1.1208],\n",
       "          [-1.6520,  0.6282,  1.2490],\n",
       "          [ 0.0000,  0.0000,  0.0000]]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ClassifyWrapper():\n",
    "\n",
    "    def __init__(self,device='cuda') -> None:\n",
    "#         self.model_name = model_name\n",
    "#         self.model, self.tokenizer = models.load_model_and_tokenizer(model_name, device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\")\n",
    "\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_pred(self, sen_1: list, sen_2: list, max_batch_size=128):\n",
    "        inputs = [_[0] + ' [SEP] ' + _[1] for _ in zip(sen_1, sen_2)]\n",
    "        inputs = self.tokenizer(inputs, padding=True, truncation=True)\n",
    "        input_ids = torch.tensor(inputs['input_ids']).to(self.model.device)\n",
    "        attention_mask = torch.tensor(inputs['attention_mask']).to(self.model.device)\n",
    "        logits = []\n",
    "        for st in range(0, len(input_ids), max_batch_size):\n",
    "            ed = min(st + max_batch_size, len(input_ids))\n",
    "            logits.append(self.model(input_ids=input_ids[st:ed],\n",
    "                                attention_mask=attention_mask[st:ed])['logits'])\n",
    "        return torch.cat(logits, dim=0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def create_sim_mat_batched(self, question, answers):\n",
    "        unique_ans = sorted(list(set(answers)))\n",
    "        semantic_set_ids = {ans: i for i, ans in enumerate(unique_ans)}\n",
    "        _rev_mapping = semantic_set_ids.copy()\n",
    "        sim_mat_batch = torch.zeros((len(unique_ans), len(unique_ans),3))\n",
    "        anss_1, anss_2, indices = [], [], []\n",
    "        for i, ans_i in enumerate(unique_ans):\n",
    "            for j, ans_j in enumerate(unique_ans):\n",
    "                if i == j: continue\n",
    "                anss_1.append(f\"{question} {ans_i}\")\n",
    "                anss_2.append(f\"{question} {ans_j}\")\n",
    "                indices.append((i,j))\n",
    "        if len(indices) > 0:\n",
    "            sim_mat_batch_flat = self._batch_pred(anss_1, anss_2)\n",
    "            for _, (i,j) in enumerate(indices):\n",
    "                sim_mat_batch[i,j] = sim_mat_batch_flat[_]\n",
    "        return dict(\n",
    "            mapping = [_rev_mapping[_] for _ in answers],\n",
    "            sim_mat = sim_mat_batch\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _pred(self, sen_1: str, sen_2: str):\n",
    "        input = sen_1 + ' [SEP] ' + sen_2\n",
    "        input_ids = self.tokenizer.encode(input, return_tensors='pt').to(self.model.device)\n",
    "\n",
    "        logits = self.model(input_ids)['logits']\n",
    "        # logits: [Contradiction, neutral, entailment]\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def pred_qa(self, question:str, ans_1:str, ans_2:str):\n",
    "        return self._pred(f\"{question} {ans_1}\", f'{question} {ans_2}')\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _compare(self, question:str, ans_1:str, ans_2:str):\n",
    "        pred_1 = self._pred(f\"{question} {ans_1}\", f'{question} {ans_2}')\n",
    "        pred_2 = self._pred(f\"{question} {ans_2}\", f'{question} {ans_1}')\n",
    "        preds = torch.concat([pred_1, pred_2], 0)\n",
    "\n",
    "        deberta_prediction = 0 if preds.argmax(1).min() == 0 else 1\n",
    "        return {'deberta_prediction': deberta_prediction,\n",
    "                'prob': torch.softmax(preds,1).mean(0).cpu(),\n",
    "                'pred': preds.cpu()\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "nli_model = ClassifyWrapper(device = device)\n",
    "sims = nli_model.create_sim_mat_batched(demo_data['question'], demo_data['generations']['text_cleaned'])\n",
    "sims\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1765ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 19, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims['sim_mat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eee989bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2445754/3139966089.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ret[i,j] = torch.tensor(sim_mat[mapping[i], mapping[j]])\n"
     ]
    }
   ],
   "source": [
    "def recover_sim_mat_new(sim):\n",
    "    sim_mat = sim['sim_mat'].clone()\n",
    "    sim_mat[torch.arange(sim_mat.shape[0]), torch.arange(sim_mat.shape[0]), :] = torch.tensor([-torch.inf, -torch.inf, 100])\n",
    "    mapping = sim['mapping']\n",
    "    # a len(ans) x len(ans) x 3 tensor\n",
    "    ret = torch.zeros((len(mapping), len(mapping), 3))\n",
    "    for i, ans_i in enumerate(mapping):\n",
    "        for j, ans_j in enumerate(mapping):\n",
    "            ret[i,j] = torch.tensor(sim_mat[mapping[i], mapping[j]])\n",
    "    return None, ret\n",
    "\n",
    "sim_new = recover_sim_mat_new(sims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91ebf9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 20, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_new[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29051748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_affinity_mat(logits, mode='disagreement', temp=None, symmetric=True):\n",
    "    \n",
    "    W = torch.softmax(logits/temp, dim=-1)[:, :, 0]\n",
    "    if symmetric:\n",
    "        W = (W + W.permute(1,0))/2\n",
    "        W = 1 - W\n",
    "\n",
    "    W = W.cpu().numpy()\n",
    "    W[np.arange(len(W)), np.arange(len(W))] = 1\n",
    "    W = W.astype(np.float32)\n",
    "    return W\n",
    "\n",
    "def get_L_mat(W, symmetric=True):\n",
    "    # compute the degree matrix from the weighted adjacency matrix\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "    # compute the normalized laplacian matrix from the degree matrix and weighted adjacency matrix\n",
    "    if symmetric:\n",
    "        L = np.linalg.inv(np.sqrt(D)) @ (D - W) @ np.linalg.inv(np.sqrt(D))\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "        # compute the normalized laplacian matrix from the degree matrix and weighted adjacency matrix\n",
    "        L = np.linalg.inv(D) @ (D - W)\n",
    "    return L.copy()\n",
    "\n",
    "def get_eig(L, thres=None, eps=None):\n",
    "    # This function assumes L is symmetric\n",
    "    # compute the eigenvalues and eigenvectors of the laplacian matrix\n",
    "    if eps is not None:\n",
    "        L = (1-eps) * L + eps * np.eye(len(L))\n",
    "    eigvals, eigvecs = np.linalg.eigh(L)\n",
    "\n",
    "    #eigvals, eigvecs = np.linalg.eig(L)\n",
    "    #assert np.max(np.abs(eigvals.imag)) < 1e-5\n",
    "    #eigvals = eigvals.real\n",
    "    #idx = eigvals.argsort()\n",
    "    #eigvals = eigvals[idx]\n",
    "    #eigvecs = eigvecs[:,idx]\n",
    "\n",
    "    if thres is not None:\n",
    "        keep_mask = eigvals < thres\n",
    "        eigvals, eigvecs = eigvals[keep_mask], eigvecs[:, keep_mask]\n",
    "    return eigvals, eigvecs\n",
    "\n",
    "\n",
    "temp_projected = clusterer.proj(sim_new[1]) \n",
    "temp_projected.shape\n",
    "#eigenvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bf011ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 20, 2),\n",
       " array([[[-0.19817235,  0.49236935],\n",
       "         [-0.2363999 ,  0.1229063 ],\n",
       "         [-0.22433715,  0.11301934],\n",
       "         [-0.20555821,  0.20639502],\n",
       "         [-0.20794784,  0.23905285],\n",
       "         [-0.22498329, -0.26364177],\n",
       "         [-0.22706062,  0.09240429],\n",
       "         [-0.23169556,  0.14002341],\n",
       "         [-0.22136311,  0.13089608],\n",
       "         [-0.20321164,  0.3568211 ],\n",
       "         [-0.23141728,  0.0911303 ],\n",
       "         [-0.23071909, -0.14955612],\n",
       "         [-0.2284697 , -0.2081573 ],\n",
       "         [-0.22766563, -0.26495525],\n",
       "         [-0.23011257,  0.09995395],\n",
       "         [-0.23132809, -0.14886685],\n",
       "         [-0.22463724, -0.18516462],\n",
       "         [-0.2253452 , -0.23721768],\n",
       "         [-0.22915488, -0.22522582],\n",
       "         [-0.22766563, -0.26495525]]], dtype=float32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_projected_ = np.reshape(temp_projected, (1, 20, 2))\n",
    "temp_projected_.shape, temp_projected_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06518c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.000622], dtype=float32),\n",
       " array([[0.4861608 , 0.11677483, 0.10616226, 0.2003262 , 0.23270239,\n",
       "         0.2705082 , 0.08562264, 0.13342233, 0.12405062, 0.35053918,\n",
       "         0.08465284, 0.15659058, 0.21507952, 0.27185088, 0.09333681,\n",
       "         0.155932  , 0.1920304 , 0.2440873 , 0.23215964, 0.27185088]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = np.asarray([np.linalg.norm(x -x.mean(0)[None, :],2,axis=1) for x in temp_projected_])\n",
    "ds_ = np.linalg.norm(ds, 2, 1)\n",
    "ds_, ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c913e92",
   "metadata": {},
   "source": [
    "number_semantic_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb1d430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "semantic_set_ids {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18}\n",
      "list_of_semantic_set_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sem_set [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of semantic set: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "CONTRADICT, NEUTRAL, AGREE = 0, 1, 2\n",
    "\n",
    "def _create_semantic_sets(sample):\n",
    "    # https://github.com/lorenzkuhn/semantic_uncertainty\n",
    "    generated_texts = sample['mapping']\n",
    "    sim_mat = sample['sim_mat'].argmax(axis=-1)\n",
    "#     print('sim_mat', sim_mat)\n",
    "    # unique_ans is also a list of integers.\n",
    "    unique_generated_texts = sorted(list(set(generated_texts)))\n",
    "    print(unique_generated_texts)\n",
    "    semantic_set_ids = {ans: i for i, ans in enumerate(unique_generated_texts)} # one id for each exact-match answer\n",
    "    print('semantic_set_ids', semantic_set_ids)\n",
    "    for i, ans_i in enumerate(unique_generated_texts):\n",
    "        for j, ans_j in enumerate(unique_generated_texts[i+1:], i+1):\n",
    "            if min(sim_mat[ans_i,ans_j], sim_mat[ans_j,ans_i]) > CONTRADICT:\n",
    "                semantic_set_ids[ans_j] = semantic_set_ids[ans_i]\n",
    "\n",
    "    list_of_semantic_set_ids = [semantic_set_ids[x] for x in generated_texts]\n",
    "    print('list_of_semantic_set_ids', list_of_semantic_set_ids)\n",
    "    # map according to the order of appearance\n",
    "    _map = defaultdict(int)\n",
    "    ret = []\n",
    "    for i, ans in enumerate(list_of_semantic_set_ids):\n",
    "        if ans not in _map:\n",
    "            _map[ans] = len(_map)\n",
    "        ret.append(_map[ans])\n",
    "    return ret\n",
    "\n",
    "num_gens = 20\n",
    "\n",
    "sem_set = _create_semantic_sets(sims)\n",
    "\n",
    "print('sem_set', sem_set)\n",
    "\n",
    "get_numsets = len(set(sem_set))\n",
    "\n",
    "print('number of semantic set:', get_numsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7c31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bc95042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mapping': [11,\n",
       "  0,\n",
       "  8,\n",
       "  2,\n",
       "  7,\n",
       "  6,\n",
       "  15,\n",
       "  3,\n",
       "  9,\n",
       "  1,\n",
       "  12,\n",
       "  10,\n",
       "  13,\n",
       "  17,\n",
       "  5,\n",
       "  16,\n",
       "  4,\n",
       "  18,\n",
       "  14,\n",
       "  17],\n",
       " 'sim_mat': tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "          [-0.3503,  1.4741, -0.9068],\n",
       "          [ 0.4314,  0.4489, -1.0398],\n",
       "          ...,\n",
       "          [-0.6590,  2.0907, -0.9428],\n",
       "          [-0.3062,  0.5733, -0.2908],\n",
       "          [-0.2184,  0.4827, -0.3478]],\n",
       " \n",
       "         [[-0.9413,  0.9582,  0.1809],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 1.7362, -0.3184, -1.7784],\n",
       "          ...,\n",
       "          [ 1.9157, -0.3024, -1.8874],\n",
       "          [ 2.9792, -1.1658, -1.9188],\n",
       "          [ 3.0403, -1.1253, -2.0292]],\n",
       " \n",
       "         [[-0.5741,  0.4342,  0.1214],\n",
       "          [ 1.2542, -0.4389, -1.2263],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.7801,  0.0574, -1.0693],\n",
       "          [ 1.1383, -0.5187, -0.9054],\n",
       "          [ 1.3437, -0.7421, -0.8874]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.7413,  0.7301,  0.0792],\n",
       "          [ 1.2882, -0.1700, -1.4613],\n",
       "          [ 1.4731, -0.1073, -1.7052],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [-1.2190,  0.5205,  0.8077],\n",
       "          [-0.9970,  0.4188,  0.6423]],\n",
       " \n",
       "         [[-0.8831,  1.1258, -0.0299],\n",
       "          [ 2.7944, -1.0997, -1.9701],\n",
       "          [ 2.8198, -1.0252, -2.0587],\n",
       "          ...,\n",
       "          [-1.4542,  3.6309, -0.9379],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [-1.5950, -0.4790,  2.1288]],\n",
       " \n",
       "         [[-0.4027,  1.0573, -0.5352],\n",
       "          [ 2.4884, -0.7383, -2.0796],\n",
       "          [ 2.9633, -0.9536, -2.2608],\n",
       "          ...,\n",
       "          [-1.1934,  3.4397, -1.1208],\n",
       "          [-1.6520,  0.6282,  1.2490],\n",
       "          [ 0.0000,  0.0000,  0.0000]]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f1a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ad244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac1731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_ids = torch.tensor([    1,  9038,  2501,   263,   931, 29892,   297,   263,  2594, 29876,\n",
    "          2978,   263, 17888,  3699, 29892,   727, 10600,   263,  2217,  4796,\n",
    "           413, 16097,  4257, 26783,   880, 29889, 26783,   880, 10600,  1880,\n",
    "           701,   297,   263,  7575, 14294,  2058,  2038,   278,  2594, 29876,\n",
    "           988,   599,   310,   278,  2215,  1050, 29915, 29879, 15100, 12844,\n",
    "           415, 29889,  1205, 26783,   880,  9007, 29915, 29873,  7432,   297,\n",
    "           902,  2217,  3271,  2038,   278,  2594, 29876, 29892,  9360,   694,\n",
    "         29889,  2296,  7258,   902, 14842,  6592,   411,   902, 16823,  1357,\n",
    "           322, 29871, 29945,   916,  9883, 29879, 29889,  2178,   310,   902,\n",
    "          9883, 29879,   892,   274,  1082,   322,  1652,  3096, 29891, 29892,\n",
    "           763, 26783,   880, 29889,  1205,  1183,   471,   278,   871,  4796,\n",
    "           697,   297,   278, 14928, 29889,   450,  1791,   310,   902,  9883,\n",
    "         29879,   892,   599, 24841,   411,  9560,  4796,   260,  4087, 10076,\n",
    "          5547,   763, 26783,   880, 29915, 29879, 16823,  1357, 29889, 28265,\n",
    "          1422,  1754, 26783,   880,  3755, 14610, 29889,  2296,  4049, 20024,\n",
    "          1183,  5148,   763,   278,  1791,   310,   902,  3942, 29889,  1105,\n",
    "           697,  2462, 29892,   746, 26783,   880,  1476,   263,   508,   310,\n",
    "           278,  2030,  2215,  1050, 29915, 29879, 24841, 10675, 29892,  1183,\n",
    "          1304,   372,   304, 10675,  8735,   763,   963, 29889,  1932,   902,\n",
    "         16823,  1357,   322,  9883, 29879,  1476,   902,   896,  4687,   425,\n",
    "         29700, 29889, 29871,    13,    13, 29908,  5618,   526,   366,  2599,\n",
    "         29892, 26783,   880, 29973,  3850, 29871,    13,    13, 29908, 29902,\n",
    "           871,  5131,   304,   367,   901,   763,   366,  1642, 29871,    13,\n",
    "            13, 29907,   327,   880, 29915, 29879, 16823,  1357, 14051,  2580,\n",
    "           902,  3700,   373, 26783,   880, 29915, 29879,   322,  1497,   376,\n",
    "          9048, 26783,   880, 29892,   541,   596,  3261,   338,   577,  5051,\n",
    "           322,  4266, 29892,   763,   366, 29889,  1334,   723,  2360,   864,\n",
    "           366,   304,   367,   738,   916,   982,  1642,  1126,   411,   393,\n",
    "         29892, 26783,   880, 29915, 29879, 16823,  1357, 18691,   902,   701,\n",
    "           322, 13700,   902,   964,   263,  4802, 20968,   310,  4094, 29889,\n",
    "          1932, 26783,   880,  2996,   714,  1183,   471,  8735,  1449, 29889,\n",
    "          2439,  9883, 29879,   301, 17840,   902,  3700,  2745, 26783,   880,\n",
    "         29915, 29879,  3261,   471,   599,   599, 15589, 29889, 29871,    13,\n",
    "            13, 29908, 10310, 29915, 29873,  3926,   437,   393,  1449, 29892,\n",
    "         26783,   880,  3850,   896,   599, 10680, 29889,   376,  9190,   931,\n",
    "           366,  1795,  4473,   701,   393,  5051,  4796,  3261,   310, 15850,\n",
    "           322,   591,  7656, 29915, 29873,   864,   393,  3850, 29871,    13,\n",
    "            13, 11760, 26783,   880,  2714, 29892,   376, 29902,  1735,   590,\n",
    "          3458, 29889,   306,   763,  1641,  4266,  1642,   660, 29901,  1724,\n",
    "          2927,   471, 26783,   880, 29973,   319, 29901])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efdd86a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([397])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f72ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
