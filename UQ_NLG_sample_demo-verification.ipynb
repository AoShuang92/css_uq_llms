{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d3de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 31 16:19:28 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P40           Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    49W / 250W |      0MiB / 23040MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P40           Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    48W / 250W |      0MiB / 23040MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bea1696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /data/user-data/sa25729/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d60af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 16:19:38.913458: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-31 16:19:39.804237: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          OPTForCausalLM)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442a03da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5fd4ddc9864b4d9282d763ca7202ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "680fe6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data = {'prompt': 'Answer these questions:\\nQ: In Scotland a bothy/bothie is a?\\nA: House\\nQ: Which 2008 Western film starred Ed Harris and Viggo Mortensen as lawmen, Jeremy Irons as a rancher and Renee Zellweger as a piano-playing widow?\\nA:',\n",
    "  'id': 'qw_8385',\n",
    "  'question': 'Which 2008 Western film starred Ed Harris and Viggo Mortensen as lawmen, Jeremy Irons as a rancher and Renee Zellweger as a piano-playing widow?',\n",
    "  'answer': ['Appaloosa'],\n",
    "  'generations': {'text_cleaned': [' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa'],\n",
    "   'text': [' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n',\n",
    "    ' Appaloosa\\n']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3021239f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict,\n",
       " dict_keys(['prompt', 'id', 'question', 'answer', 'generations']),\n",
       " dict_keys(['text_cleaned', 'text']))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(demo_data), demo_data.keys() ,demo_data['generations'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f03cfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 76]), torch.Size([1, 76]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ids = tokenizer(demo_data['prompt'], truncation=False, padding=False)\n",
    "\n",
    "input_ids = torch.tensor(demo_ids['input_ids'])\n",
    "input_ids = input_ids[None, :]\n",
    "atten_mask = torch.tensor(demo_ids['attention_mask'])\n",
    "atten_mask = atten_mask[None, :]\n",
    "\n",
    "input_ids.size(), atten_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a8a5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_config(tokenizer):\n",
    "\n",
    "#     if tokenizer.__class__.__name__ == 'LlamaTokenizerFast':\n",
    "#         eos_token_id = [tokenizer.encode(_)[-1] for _ in ['.', '\\n']] + [29889]  # seems to be '.' as well\n",
    "#         #eos_token_id = [tokenizer(_)['input_ids'] for _ in ['\\n', ',', '.']]\n",
    "#     elif tokenizer.__class__.__name__ == 'GPT2Tokenizer':\n",
    "#         eos_token_id = [tokenizer.encode(_)[1] for _ in ['.', '\\n']]\n",
    "#     else:\n",
    "#         raise NotImplementedError\n",
    "\n",
    "    eos_token_id = [tokenizer.encode(_)[-1] for _ in ['.', '\\n']] + [29889]  # seems to be '.' as well\n",
    "    eos_token_id += [tokenizer.eos_token_id]\n",
    "    question_framing_ids = ['Question:', ' Question:', '\\n', 'Answer:', ' Answer:', 'Q:']\n",
    "    # Follows Kuhn et al 2023 as Llama does not have CoQA\n",
    "    question_framing_ids = [[tokenizer(eos_token)['input_ids'][1]] for eos_token in question_framing_ids]\n",
    "    return dict(eos_token_id=eos_token_id, bad_words_ids=question_framing_ids)\n",
    "\n",
    "def get_generation_config(input_ids, tokenizer):\n",
    "    assert len(input_ids.shape) == 2\n",
    "    max_length_of_generated_sequence = 256\n",
    "#     if data_name == 'triviaqa':\n",
    "#         generation_config = triviaqa._generate_config(tokenizer)\n",
    "#     if data_name == 'coqa':\n",
    "#         generation_config = coqa._generate_config(tokenizer)\n",
    "#     if data_name == 'nq_open':\n",
    "#         generation_config = nq_open._generate_config(tokenizer)\n",
    "\n",
    "    generation_config = _generate_config(tokenizer)\n",
    "    generation_config['max_new_tokens'] = max_length_of_generated_sequence\n",
    "    generation_config['early_stopping'] = True\n",
    "    # https://jaketae.github.io/study/gpt2/#setup\n",
    "    generation_config['pad_token_id'] = tokenizer.eos_token_id\n",
    "    return generation_config\n",
    "\n",
    "generation_config = get_generation_config(input_ids, tokenizer)\n",
    "generation_config = transformers.GenerationConfig(**generation_config)\n",
    "input_length = input_ids.shape[1]\n",
    "\n",
    "most_likely_generations = model.generate(input_ids, attention_mask = atten_mask,\n",
    "                                                    num_beams=1,\n",
    "                                                    do_sample=False,\n",
    "                                                    generation_config=generation_config).cpu()[0, input_length:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b9595e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2401, 7003, 3628,   13])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['App', 'alo', 'osa', '\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(most_likely_generations)\n",
    "# most_likely_generations tensor([26783,   880,   471,  4796, 29889])\n",
    "\n",
    "generated_texts = [tokenizer.decode(_, skip_special_tokens=True) for _ in most_likely_generations]\n",
    "generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c01902d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[2401, 7003, 3628,   13],\n",
       "         [2401, 7003, 3628,   13],\n",
       "         [2401, 7003, 3628,   13],\n",
       "         [2401, 7003, 3628,   13]]),\n",
       " tensor([[2401, 7003, 3628,   13],\n",
       "         [2401, 7003, 3628,   13],\n",
       "         [2401, 7003, 3628,   13],\n",
       "         [2401, 7003, 3628,    2]]),\n",
       " tensor([[2401, 7003, 3628,   13],\n",
       "         [2401, 7003, 3628,   13],\n",
       "         [2401, 7003, 3628,   13],\n",
       "         [2401, 7003, 3628,   13]]),\n",
       " tensor([[2401, 7003, 3628,   13],\n",
       "         [2401, 7003, 3628,   13],\n",
       "         [2401, 7003, 3628,   13],\n",
       "         [2401, 7003, 3628,   13]]),\n",
       " tensor([[ 2401,  7003,  3628, 29889],\n",
       "         [ 2401,  7003,  3628,    13],\n",
       "         [ 2401,  7003,  3628,    13],\n",
       "         [ 2401,  7003,  3628,    13]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations = []\n",
    "num_gens = 20\n",
    "max_num_gen_once=4\n",
    "while num_gens > 0:\n",
    "    _ =  model.generate(input_ids, attention_mask=atten_mask,\n",
    "                             num_beams=1, num_return_sequences=min(max_num_gen_once, num_gens),\n",
    "                             do_sample=True, top_p= 1, top_k= 0,\n",
    "                             temperature=1, generation_config=generation_config,\n",
    "                             )\n",
    "    generations.append(_[:, input_length:].cpu())\n",
    "    num_gens -= len(_)\n",
    "    \n",
    "generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac0212d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['App', 'alo', 'osa', '\\n'], 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_gen = generations[2][0]\n",
    "generated_sample = [tokenizer.decode(_, skip_special_tokens=True) for _ in sample_gen]\n",
    "generated_sample, len(sample_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35d49653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['App', 'alo', 'osa', '\\n'], 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_gen = generations[4][2]\n",
    "generated_sample = [tokenizer.decode(_, skip_special_tokens=True) for _ in sample_gen]\n",
    "generated_sample, len(sample_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93bcafe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b7f93e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2444252/4149883299.py:1: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
      "  generations = torch.nested.nested_tensor(generations).to_padded_tensor(tokenizer.eos_token_id)\n"
     ]
    }
   ],
   "source": [
    "generations = torch.nested.nested_tensor(generations).to_padded_tensor(tokenizer.eos_token_id)\n",
    "generations = generations.reshape(-1, generations.shape[-1])[:20]\n",
    "generated_texts = [tokenizer.decode(_, skip_special_tokens=True) for _ in generations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbe85836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa.',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n',\n",
       " 'Appaloosa\\n']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbce5ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences 1 [{'prompt': tensor([[    1,   673,  1438,  5155, 29901,    13, 29984, 29901,   512, 15226,\n",
      "           263,  1716, 29891, 29914, 20313,   347,   338,   263, 29973,    13,\n",
      "         29909, 29901,  5619,    13, 29984, 29901,  8449, 29871, 29906, 29900,\n",
      "         29900, 29947, 10504,  2706,  5810,  1127,  2155, 20349,   322,   478,\n",
      "           335,  1484, 15533, 14762,   408,  4307,  1527, 29892,  5677,  6764,\n",
      "          6600,   787,   408,   263,  6350,  4630,   322,   390,  1600, 29872,\n",
      "           796,   514,  7872,   261,   408,   263, 11914, 29899,  1456,   292,\n",
      "          9449,   340, 29973,    13, 29909, 29901]]), 'id': '3dr23u6we5exclen4th8uq9rb42tel_0', 'question': 'What color was Cotton?', 'answer': 'white', 'additional_answers': [], 'most_likely_generation_ids': tensor([2401, 7003, 3628,   13]), 'generations_ids': tensor([[ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,     2],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628, 29889],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13],\n",
      "        [ 2401,  7003,  3628,    13]]), 'most_likely_generation': 'Appaloosa\\n', 'generations': ['Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa.', 'Appaloosa\\n', 'Appaloosa\\n', 'Appaloosa\\n']}]\n"
     ]
    }
   ],
   "source": [
    "# remember the data\n",
    "# id 3dr23u6we5exclen4th8uq9rb42tel_0\n",
    "# que What color was Cotton?\n",
    "# ans white\n",
    "\n",
    "sequences = []\n",
    "curr_seq = dict(\n",
    "    prompt= input_ids,\n",
    "    id='3dr23u6we5exclen4th8uq9rb42tel_0',\n",
    "    question= 'What color was Cotton?',\n",
    "    answer= 'white',\n",
    "    additional_answers= [],\n",
    ")\n",
    "curr_seq.update(\n",
    "    dict(\n",
    "        most_likely_generation_ids = most_likely_generations,\n",
    "        generations_ids=generations,\n",
    "    )\n",
    ")\n",
    "curr_seq.update(\n",
    "    dict(\n",
    "        most_likely_generation=tokenizer.decode(curr_seq['most_likely_generation_ids'], skip_special_tokens=True),\n",
    "        generations=generated_texts,\n",
    "    )\n",
    ")\n",
    "\n",
    "# curr_seq['additional_answers'] = [x[0] for x in additional_answers]\n",
    "\n",
    "sequences.append(curr_seq)\n",
    "print('sequences', len(sequences), sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49beb62b",
   "metadata": {},
   "source": [
    "Calculate Uncertainty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f4499ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install rouge_score\n",
    "!pip install -q evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda1dfd6",
   "metadata": {},
   "source": [
    "RougeL score to get uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "047782de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n",
      "predictions <class 'list'> ['appaloosa']\n",
      "references <class 'list'> ['appaloosa']\n",
      "{'exact_match': 1.0, 'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge', keep_in_memory=True)\n",
    "exact_match_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "def _compare_generated_text_to_answers(pred_txt, reference_answers):\n",
    "    pred_txt = pred_txt.lstrip().lower()\n",
    "    rouge_types = ['rouge1', 'rouge2', 'rougeL']\n",
    "    sequence_dict = {_:0. for _ in ['exact_match'] + rouge_types}\n",
    "    unique_reference_answers = set([_.lstrip().lower() for _ in reference_answers])\n",
    "    for answer in unique_reference_answers:\n",
    "        predictions = [pred_txt]\n",
    "        print('predictions', type(predictions), predictions)\n",
    "        references = [answer]\n",
    "        print('references', type(references), references)\n",
    "        results = exact_match_metric.compute(predictions=predictions,\n",
    "                                             references=references,\n",
    "                                             ignore_case=True,\n",
    "                                             ignore_punctuation=True)\n",
    "        sequence_dict['exact_match'] = max(results['exact_match'], sequence_dict['exact_match'])\n",
    "        rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "        for rouge_type in rouge_types:\n",
    "            sequence_dict[rouge_type] = max(rouge_results[rouge_type], sequence_dict[rouge_type])\n",
    "    return sequence_dict\n",
    "\n",
    "all_sequences = []\n",
    "all_rougeL = []\n",
    "for i in range(len(demo_data['generations']['text_cleaned'])):\n",
    "    sequences_rouge = _compare_generated_text_to_answers(demo_data['generations']['text_cleaned'][i], demo_data['answer'])\n",
    "    print(sequences_rouge)\n",
    "    all_sequences.append(sequences_rouge)\n",
    "    all_rougeL.append(sequences_rouge['rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b471f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rouge_score = np.array(all_rougeL).mean()\n",
    "rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba89eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "308177d1",
   "metadata": {},
   "source": [
    "Eccentricity for negtive confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02ca245e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def jaccard_one(all_answers):\n",
    "    all_answers = [set(ans.lower().split()) for ans in all_answers]\n",
    "    ret = np.eye(len(all_answers))\n",
    "    for i, ans_i in enumerate(all_answers):\n",
    "        for j, ans_j in enumerate(all_answers[i+1:], i+1):\n",
    "            ret[i,j] = ret[j,i] = len(ans_i.intersection(ans_j)) / max(len(ans_i.union(ans_j)),1)\n",
    "            \n",
    "    return ret\n",
    "\n",
    "ret = jaccard_one(demo_temp['text_cleaned'])\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "191030c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpetralClusteringFromLogits:\n",
    "    def __init__(self,\n",
    "                 affinity_mode='disagreement_w',\n",
    "                 eigv_threshold=0.9,\n",
    "                 cluster=True,\n",
    "                 temperature=3., adjust=False) -> None:\n",
    "        self.affinity_mode = affinity_mode\n",
    "        self.eigv_threshold = eigv_threshold\n",
    "        self.rs = 0\n",
    "        self.cluster = cluster\n",
    "        self.temperature = temperature\n",
    "        self.adjust = adjust\n",
    "        if affinity_mode == 'jaccard':\n",
    "            assert self.temperature is None\n",
    "\n",
    "    def get_laplacian(self, logits):\n",
    "        W = get_affinity_mat(logits, mode=self.affinity_mode, temp=self.temperature)\n",
    "        L = get_L_mat(W, symmetric=True)\n",
    "        return L\n",
    "\n",
    "    def get_eigvs(self, logits):\n",
    "        L = self.get_laplacian(logits)\n",
    "        return (1-get_eig(L)[0])\n",
    "\n",
    "    def __call__(self, logits, cluster=None):\n",
    "        if cluster is None: cluster = self.cluster\n",
    "        L = self.get_laplacian(logits)\n",
    "        if not cluster:\n",
    "            return (1-get_eig(L)[0]).clip(0 if self.adjust else -1).sum()\n",
    "        eigvals, eigvecs = get_eig(L, thres=self.eigv_threshold)\n",
    "        k = eigvecs.shape[1]\n",
    "        self.rs += 1\n",
    "        kmeans = KMeans(n_clusters=k, random_state=self.rs, n_init='auto').fit(eigvecs)\n",
    "        return kmeans.labels_\n",
    "\n",
    "    def clustered_entropy(self, logits):\n",
    "        from scipy.stats import entropy\n",
    "        labels = self(logits, cluster=True)\n",
    "        P = torch.softmax(logits, dim=-1)[:, :, 2].cpu().numpy()\n",
    "        pi = find_equidist(P)\n",
    "        clustered_pi = pd.Series(pi).groupby(labels).sum().values\n",
    "        return entropy(clustered_pi)\n",
    "\n",
    "    def eig_entropy(self, logits):\n",
    "        W = get_affinity_mat(logits, mode=self.affinity_mode, temp=self.temperature)\n",
    "        L = get_L_mat(W, symmetric=True)\n",
    "        eigs = get_eig(L, eps=1e-4)[0] / W.shape[0]\n",
    "        return np.exp(- (eigs * np.nan_to_num(np.log(eigs))).sum())\n",
    "\n",
    "    def proj(self, logits):\n",
    "        W = get_affinity_mat(logits, mode=self.affinity_mode, temp=self.temperature)\n",
    "        L = get_L_mat(W, symmetric=True)\n",
    "        eigvals, eigvecs = get_eig(L, thres=self.eigv_threshold)\n",
    "        return eigvecs\n",
    "\n",
    "    def kmeans(self, eigvecs):\n",
    "        k = eigvecs.shape[1]\n",
    "        self.rs += 1\n",
    "        kmeans = KMeans(n_clusters=k, random_state=self.rs, n_init='auto').fit(eigvecs)\n",
    "        return kmeans.labels_\n",
    "\n",
    "def umap_visualization(eigvecs, labels):\n",
    "    # perform umap visualization on the eigenvectors\n",
    "    import umap\n",
    "    reducer = umap.UMAP()\n",
    "    embedding = reducer.fit_transform(eigvecs)\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], c=labels)\n",
    "    return embedding\n",
    "\n",
    "clusterer = SpetralClusteringFromLogits(affinity_mode='agreement_w', eigv_threshold=0.9,\n",
    "                                                       cluster=False, temperature=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5af418a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mapping': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'sim_mat': tensor([[[0., 0., 0.]]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ClassifyWrapper():\n",
    "\n",
    "    def __init__(self,device='cuda') -> None:\n",
    "#         self.model_name = model_name\n",
    "#         self.model, self.tokenizer = models.load_model_and_tokenizer(model_name, device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\")\n",
    "\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_pred(self, sen_1: list, sen_2: list, max_batch_size=128):\n",
    "        inputs = [_[0] + ' [SEP] ' + _[1] for _ in zip(sen_1, sen_2)]\n",
    "        inputs = self.tokenizer(inputs, padding=True, truncation=True)\n",
    "        input_ids = torch.tensor(inputs['input_ids']).to(self.model.device)\n",
    "        attention_mask = torch.tensor(inputs['attention_mask']).to(self.model.device)\n",
    "        logits = []\n",
    "        for st in range(0, len(input_ids), max_batch_size):\n",
    "            ed = min(st + max_batch_size, len(input_ids))\n",
    "            logits.append(self.model(input_ids=input_ids[st:ed],\n",
    "                                attention_mask=attention_mask[st:ed])['logits'])\n",
    "        return torch.cat(logits, dim=0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def create_sim_mat_batched(self, question, answers):\n",
    "        unique_ans = sorted(list(set(answers)))\n",
    "        semantic_set_ids = {ans: i for i, ans in enumerate(unique_ans)}\n",
    "        _rev_mapping = semantic_set_ids.copy()\n",
    "        sim_mat_batch = torch.zeros((len(unique_ans), len(unique_ans),3))\n",
    "        anss_1, anss_2, indices = [], [], []\n",
    "        for i, ans_i in enumerate(unique_ans):\n",
    "            for j, ans_j in enumerate(unique_ans):\n",
    "                if i == j: continue\n",
    "                anss_1.append(f\"{question} {ans_i}\")\n",
    "                anss_2.append(f\"{question} {ans_j}\")\n",
    "                indices.append((i,j))\n",
    "        if len(indices) > 0:\n",
    "            sim_mat_batch_flat = self._batch_pred(anss_1, anss_2)\n",
    "            for _, (i,j) in enumerate(indices):\n",
    "                sim_mat_batch[i,j] = sim_mat_batch_flat[_]\n",
    "        return dict(\n",
    "            mapping = [_rev_mapping[_] for _ in answers],\n",
    "            sim_mat = sim_mat_batch\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _pred(self, sen_1: str, sen_2: str):\n",
    "        input = sen_1 + ' [SEP] ' + sen_2\n",
    "        input_ids = self.tokenizer.encode(input, return_tensors='pt').to(self.model.device)\n",
    "\n",
    "        logits = self.model(input_ids)['logits']\n",
    "        # logits: [Contradiction, neutral, entailment]\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def pred_qa(self, question:str, ans_1:str, ans_2:str):\n",
    "        return self._pred(f\"{question} {ans_1}\", f'{question} {ans_2}')\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _compare(self, question:str, ans_1:str, ans_2:str):\n",
    "        pred_1 = self._pred(f\"{question} {ans_1}\", f'{question} {ans_2}')\n",
    "        pred_2 = self._pred(f\"{question} {ans_2}\", f'{question} {ans_1}')\n",
    "        preds = torch.concat([pred_1, pred_2], 0)\n",
    "\n",
    "        deberta_prediction = 0 if preds.argmax(1).min() == 0 else 1\n",
    "        return {'deberta_prediction': deberta_prediction,\n",
    "                'prob': torch.softmax(preds,1).mean(0).cpu(),\n",
    "                'pred': preds.cpu()\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "nli_model = ClassifyWrapper(device = device)\n",
    "sims = nli_model.create_sim_mat_batched(demo_data['question'], demo_data['generations']['text_cleaned'])\n",
    "sims\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1765ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims['sim_mat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eee989bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2444252/3139966089.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ret[i,j] = torch.tensor(sim_mat[mapping[i], mapping[j]])\n"
     ]
    }
   ],
   "source": [
    "def recover_sim_mat_new(sim):\n",
    "    sim_mat = sim['sim_mat'].clone()\n",
    "    sim_mat[torch.arange(sim_mat.shape[0]), torch.arange(sim_mat.shape[0]), :] = torch.tensor([-torch.inf, -torch.inf, 100])\n",
    "    mapping = sim['mapping']\n",
    "    # a len(ans) x len(ans) x 3 tensor\n",
    "    ret = torch.zeros((len(mapping), len(mapping), 3))\n",
    "    for i, ans_i in enumerate(mapping):\n",
    "        for j, ans_j in enumerate(mapping):\n",
    "            ret[i,j] = torch.tensor(sim_mat[mapping[i], mapping[j]])\n",
    "    return None, ret\n",
    "\n",
    "sim_new = recover_sim_mat_new(sims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91ebf9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 20, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_new[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29051748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_affinity_mat(logits, mode='disagreement', temp=None, symmetric=True):\n",
    "    \n",
    "    W = torch.softmax(logits/temp, dim=-1)[:, :, 0]\n",
    "    if symmetric:\n",
    "        W = (W + W.permute(1,0))/2\n",
    "        W = 1 - W\n",
    "\n",
    "    W = W.cpu().numpy()\n",
    "    W[np.arange(len(W)), np.arange(len(W))] = 1\n",
    "    W = W.astype(np.float32)\n",
    "    return W\n",
    "\n",
    "def get_L_mat(W, symmetric=True):\n",
    "    # compute the degree matrix from the weighted adjacency matrix\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "    # compute the normalized laplacian matrix from the degree matrix and weighted adjacency matrix\n",
    "    if symmetric:\n",
    "        L = np.linalg.inv(np.sqrt(D)) @ (D - W) @ np.linalg.inv(np.sqrt(D))\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "        # compute the normalized laplacian matrix from the degree matrix and weighted adjacency matrix\n",
    "        L = np.linalg.inv(D) @ (D - W)\n",
    "    return L.copy()\n",
    "\n",
    "def get_eig(L, thres=None, eps=None):\n",
    "    # This function assumes L is symmetric\n",
    "    # compute the eigenvalues and eigenvectors of the laplacian matrix\n",
    "    if eps is not None:\n",
    "        L = (1-eps) * L + eps * np.eye(len(L))\n",
    "    eigvals, eigvecs = np.linalg.eigh(L)\n",
    "\n",
    "    #eigvals, eigvecs = np.linalg.eig(L)\n",
    "    #assert np.max(np.abs(eigvals.imag)) < 1e-5\n",
    "    #eigvals = eigvals.real\n",
    "    #idx = eigvals.argsort()\n",
    "    #eigvals = eigvals[idx]\n",
    "    #eigvecs = eigvecs[:,idx]\n",
    "\n",
    "    if thres is not None:\n",
    "        keep_mask = eigvals < thres\n",
    "        eigvals, eigvecs = eigvals[keep_mask], eigvecs[:, keep_mask]\n",
    "    return eigvals, eigvecs\n",
    "\n",
    "\n",
    "temp_projected = clusterer.proj(sim_new[1]) \n",
    "temp_projected.shape\n",
    "#eigenvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1bf011ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 20, 1),\n",
       " array([[[-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068],\n",
       "         [-0.2236068]]], dtype=float32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_projected_ = np.reshape(temp_projected, (1, 20, 1))\n",
    "temp_projected_.shape, temp_projected_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06518c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.664002e-08], dtype=float32),\n",
       " array([[1.4901161e-08, 1.4901161e-08, 1.4901161e-08, 1.4901161e-08,\n",
       "         1.4901161e-08, 1.4901161e-08, 1.4901161e-08, 1.4901161e-08,\n",
       "         1.4901161e-08, 1.4901161e-08, 1.4901161e-08, 1.4901161e-08,\n",
       "         1.4901161e-08, 1.4901161e-08, 1.4901161e-08, 1.4901161e-08,\n",
       "         1.4901161e-08, 1.4901161e-08, 1.4901161e-08, 1.4901161e-08]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = np.asarray([np.linalg.norm(x -x.mean(0)[None, :],2,axis=1) for x in temp_projected_])\n",
    "ds_ = np.linalg.norm(ds, 2, 1)\n",
    "ds_, ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95abaf",
   "metadata": {},
   "source": [
    "number_semantic_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb1d430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sem_set [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of semantic set: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def _create_semantic_sets(sample):\n",
    "    # https://github.com/lorenzkuhn/semantic_uncertainty\n",
    "    generated_texts = sample['mapping']\n",
    "    sim_mat = sample['sim_mat'].argmax(axis=-1)\n",
    "    # unique_ans is also a list of integers.\n",
    "    unique_generated_texts = sorted(list(set(generated_texts)))\n",
    "    semantic_set_ids = {ans: i for i, ans in enumerate(unique_generated_texts)} # one id for each exact-match answer\n",
    "    for i, ans_i in enumerate(unique_generated_texts):\n",
    "        for j, ans_j in enumerate(unique_generated_texts[i+1:], i+1):\n",
    "            if min(sim_mat[ans_i,ans_j], sim_mat[ans_j,ans_i]) > CONTRADICT:\n",
    "                semantic_set_ids[ans_j] = semantic_set_ids[ans_i]\n",
    "\n",
    "    list_of_semantic_set_ids = [semantic_set_ids[x] for x in generated_texts]\n",
    "    # map according to the order of appearance\n",
    "    _map = defaultdict(int)\n",
    "    ret = []\n",
    "    for i, ans in enumerate(list_of_semantic_set_ids):\n",
    "        if ans not in _map:\n",
    "            _map[ans] = len(_map)\n",
    "        ret.append(_map[ans])\n",
    "    return ret\n",
    "\n",
    "num_gens = 20\n",
    "\n",
    "sem_set = _create_semantic_sets(sims)\n",
    "\n",
    "print('sem_set', sem_set)\n",
    "\n",
    "get_numsets = len(set(sem_set))\n",
    "\n",
    "print('number of semantic set:', get_numsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4441f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c009c20a",
   "metadata": {},
   "source": [
    "lexical_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb8c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360e4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498611b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419dd288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9235147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248365fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f861644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ad244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac1731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_ids = torch.tensor([    1,  9038,  2501,   263,   931, 29892,   297,   263,  2594, 29876,\n",
    "          2978,   263, 17888,  3699, 29892,   727, 10600,   263,  2217,  4796,\n",
    "           413, 16097,  4257, 26783,   880, 29889, 26783,   880, 10600,  1880,\n",
    "           701,   297,   263,  7575, 14294,  2058,  2038,   278,  2594, 29876,\n",
    "           988,   599,   310,   278,  2215,  1050, 29915, 29879, 15100, 12844,\n",
    "           415, 29889,  1205, 26783,   880,  9007, 29915, 29873,  7432,   297,\n",
    "           902,  2217,  3271,  2038,   278,  2594, 29876, 29892,  9360,   694,\n",
    "         29889,  2296,  7258,   902, 14842,  6592,   411,   902, 16823,  1357,\n",
    "           322, 29871, 29945,   916,  9883, 29879, 29889,  2178,   310,   902,\n",
    "          9883, 29879,   892,   274,  1082,   322,  1652,  3096, 29891, 29892,\n",
    "           763, 26783,   880, 29889,  1205,  1183,   471,   278,   871,  4796,\n",
    "           697,   297,   278, 14928, 29889,   450,  1791,   310,   902,  9883,\n",
    "         29879,   892,   599, 24841,   411,  9560,  4796,   260,  4087, 10076,\n",
    "          5547,   763, 26783,   880, 29915, 29879, 16823,  1357, 29889, 28265,\n",
    "          1422,  1754, 26783,   880,  3755, 14610, 29889,  2296,  4049, 20024,\n",
    "          1183,  5148,   763,   278,  1791,   310,   902,  3942, 29889,  1105,\n",
    "           697,  2462, 29892,   746, 26783,   880,  1476,   263,   508,   310,\n",
    "           278,  2030,  2215,  1050, 29915, 29879, 24841, 10675, 29892,  1183,\n",
    "          1304,   372,   304, 10675,  8735,   763,   963, 29889,  1932,   902,\n",
    "         16823,  1357,   322,  9883, 29879,  1476,   902,   896,  4687,   425,\n",
    "         29700, 29889, 29871,    13,    13, 29908,  5618,   526,   366,  2599,\n",
    "         29892, 26783,   880, 29973,  3850, 29871,    13,    13, 29908, 29902,\n",
    "           871,  5131,   304,   367,   901,   763,   366,  1642, 29871,    13,\n",
    "            13, 29907,   327,   880, 29915, 29879, 16823,  1357, 14051,  2580,\n",
    "           902,  3700,   373, 26783,   880, 29915, 29879,   322,  1497,   376,\n",
    "          9048, 26783,   880, 29892,   541,   596,  3261,   338,   577,  5051,\n",
    "           322,  4266, 29892,   763,   366, 29889,  1334,   723,  2360,   864,\n",
    "           366,   304,   367,   738,   916,   982,  1642,  1126,   411,   393,\n",
    "         29892, 26783,   880, 29915, 29879, 16823,  1357, 18691,   902,   701,\n",
    "           322, 13700,   902,   964,   263,  4802, 20968,   310,  4094, 29889,\n",
    "          1932, 26783,   880,  2996,   714,  1183,   471,  8735,  1449, 29889,\n",
    "          2439,  9883, 29879,   301, 17840,   902,  3700,  2745, 26783,   880,\n",
    "         29915, 29879,  3261,   471,   599,   599, 15589, 29889, 29871,    13,\n",
    "            13, 29908, 10310, 29915, 29873,  3926,   437,   393,  1449, 29892,\n",
    "         26783,   880,  3850,   896,   599, 10680, 29889,   376,  9190,   931,\n",
    "           366,  1795,  4473,   701,   393,  5051,  4796,  3261,   310, 15850,\n",
    "           322,   591,  7656, 29915, 29873,   864,   393,  3850, 29871,    13,\n",
    "            13, 11760, 26783,   880,  2714, 29892,   376, 29902,  1735,   590,\n",
    "          3458, 29889,   306,   763,  1641,  4266,  1642,   660, 29901,  1724,\n",
    "          2927,   471, 26783,   880, 29973,   319, 29901])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efdd86a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([397])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f72ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
